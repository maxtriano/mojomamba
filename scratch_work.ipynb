{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[2042135635, 1345947507, 2073208346, 3565425363]], dtype=uint32, shape=4)\n"
     ]
    }
   ],
   "source": [
    "from tensor import Tensor, TensorSpec, TensorShape\n",
    "from utils.index import Index\n",
    "from algorithm import vectorize\n",
    "from sys.info import simdwidthof\n",
    "from random import rand\n",
    "import math\n",
    "from python import Python\n",
    "let json = Python.import_module(\"json\")\n",
    "\n",
    "\"\"\"\n",
    "    B: batch size                       (`B` in Mamba paper [1] Algorithm 2)\n",
    "    L: sequence length                  (`L` in [1] Algorithm 2)\n",
    "    D_MODEL: hidden dim\n",
    "    D_STATE: latent state dim           (`N` in [1] Algorithm 2)\n",
    "    EXPAND: expansion factor            (`E` in [1] Section 3.4)\n",
    "    D_INNER: D_STATE * EXPAND           (`D` in [1] Algorithm 2)\n",
    "    A, B, C, D: state space parameters  (See any state space representation formula)\n",
    "                                        (B, C are input-dependent (aka selective, a key innovation in Mamba); A, D are not)\n",
    "    Δ or delta: input-dependent step size\n",
    "    dt_rank: rank of Δ                  (See [1] Section 3.6 \"Parameterization of ∆\")\n",
    "\n",
    "\"\"\"\n",
    "let B = 4\n",
    "let L = 128\n",
    "let D_MODEL = 512\n",
    "let D_STATE = 128\n",
    "let EXPAND = 4\n",
    "let D_INNER = D_STATE * EXPAND\n",
    "\n",
    "let input_ids = rand[DType.uint32](4)\n",
    "print(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor([[0.51489287614822388, 0.78978455066680908, 0.544272780418396, 0.093629911541938782]], dtype=float32, shape=4)\n"
     ]
    }
   ],
   "source": [
    "# TODO: implement\n",
    "fn get_embedding(input_ids: Tensor[DType.uint32]) -> Tensor[DType.float32]:\n",
    "    # input_ids:  Tensor of shape (b, 1)\n",
    "    # return:     Tensor of shape (b, 1, d_model)\n",
    "    let EMBEDDING_DIM = 4\n",
    "    return rand[DType.float32](EMBEDDING_DIM)\n",
    "\n",
    "print(get_embedding(input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "alias floattensor = Tensor[DType.float32]\n",
    "fn naive_matmul(A: floattensor, B: floattensor) -> floattensor:\n",
    "    var output = Tensor[DType.float32](A.shape()[0], B.shape()[1])\n",
    "    for i in range(A.shape()[0]):\n",
    "        for j in range(B.shape()[1]):\n",
    "            for k in range(A.shape()[1]):\n",
    "                output[i][j] += A[i][k] * B[k][j]\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct LinearLayer:\n",
    "    var D_IN: Int\n",
    "    var D_OUT: Int\n",
    "    var weights: Tensor[DType.float32]\n",
    "    var bias: Tensor[DType.float32]\n",
    "    var add_bias: Bool\n",
    "    \n",
    "    fn __init__(inout self, in_features: Int, out_features: Int, add_bias: Bool = False):\n",
    "        self.D_IN = in_features\n",
    "        self.D_OUT = out_features\n",
    "        self.weights = rand[DType.float32](self.D_IN, self.D_OUT)\n",
    "        self.bias = rand[DType.float32](self.D_OUT, 1)\n",
    "        self.add_bias = add_bias\n",
    "    \n",
    "    fn forward(self, input: Tensor[DType.float32]) -> Tensor[DType.float32]:\n",
    "        let output: Tensor[DType.float32] = naive_matmul(self.weights, input)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Mamba:\n",
    "    var in_projection: LinearLayer\n",
    "    var x_projection: LinearLayer\n",
    "    var dt_projection: LinearLayer\n",
    "    var out_projection: LinearLayer\n",
    "\n",
    "    fn __init__(inout self):\n",
    "        self.in_projection = LinearLayer()\n",
    "        self.x_projection = LinearLayer()\n",
    "        self.dt_projection = LinearLayer()\n",
    "        self.out_projection = LinearLayer()\n",
    "        \n",
    "    self.in_proj = nn.Linear(args.d_model, args.d_inner * 2, bias=args.bias)\n",
    "\n",
    "        self.conv1d = nn.Conv1d(\n",
    "            in_channels=args.d_inner,\n",
    "            out_channels=args.d_inner,\n",
    "            bias=args.conv_bias,\n",
    "            kernel_size=args.d_conv,\n",
    "            groups=args.d_inner,\n",
    "            padding=args.d_conv - 1,\n",
    "        )\n",
    "\n",
    "        # x_proj takes in `x` and outputs the input-specific Δ, B, C\n",
    "        self.x_proj = nn.Linear(args.d_inner, args.dt_rank + args.d_state * 2, bias=False)\n",
    "        \n",
    "        # dt_proj projects Δ from dt_rank to d_in\n",
    "        self.dt_proj = nn.Linear(args.dt_rank, args.d_inner, bias=True)\n",
    "\n",
    "        A = repeat(torch.arange(1, args.d_state + 1), 'n -> d n', d=args.d_inner)\n",
    "        self.A_log = nn.Parameter(torch.log(A))\n",
    "        self.D = nn.Parameter(torch.ones(args.d_inner))\n",
    "        self.out_proj = nn.Linear(args.d_inner, args.d_model, bias=args.bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement this\n",
    "fn compute_layer(input: Tensor[DType.float32]) -> Tensor[DType.float32]:\n",
    "    # return a random f32 tensor for now\n",
    "    return rand[DType.float32](input.shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn RMS_norm(input: Tensor[DType.float32]) -> Tensor[DType.float32]:\n",
    "    var mean: Float32 = 0.0\n",
    "    var squares = Tensor[DType.float32](input.shape())\n",
    "    var output = Tensor[DType.float32](input.shape())\n",
    "    alias simd_float32_width: Int = simdwidthof[DType.float32]()\n",
    "    \n",
    "    # Vectorized squaring of input\n",
    "    @parameter\n",
    "    fn square_tensor[simd_float32_width: Int](idx: Int) -> None:\n",
    "        squares.simd_store[simd_float32_width]\n",
    "                (idx, math.pow(input.simd_load[simd_float32_width](idx), 2))\n",
    "    vectorize[simd_float32_width, square_tensor](input.num_elements())\n",
    "\n",
    "    # Sum up squares\n",
    "    @parameter\n",
    "    fn sum_squares[simd_float32_width: Int](idx: Int) -> None:\n",
    "        mean += squares.simd_load[simd_float32_width](idx).reduce_add()\n",
    "    vectorize[simd_float32_width, sum_squares](squares.num_elements())\n",
    "\n",
    "    mean /= squares.num_elements()\n",
    "    \n",
    "    let rms: Float32 = math.sqrt(mean)\n",
    "    @parameter\n",
    "    fn divide_by_rms[simd_float32_width: Int](idx: Int) -> None:\n",
    "        output.simd_store[simd_float32_width](idx, input.simd_load[simd_float32_width](idx) / rms)\n",
    "    vectorize[simd_float32_width, divide_by_rms](output.num_elements())\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn in_projection(input: Tensor[DType.float32]) -> Tensor[DType.float32]:\n",
    "    # input: Tensor of shape (b, 1, d_model)\n",
    "    # return: Tensor of shape (b, 1, d_model)\n",
    "    let d_model = input.shape()[2]\n",
    "    let W = rand[DType.float32](TensorShape(d_model, d_model))\n",
    "    return input.matmul(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mamba block as shown in Fig. 3 in Section 3.4 of the paper\n",
    "fn compute_mamba_block(input: Tensor[DType.float32]) -> Tensor[DType.float32]:\n",
    "    let b: Int = input.shape()[0]\n",
    "    let l: Int = input.shape()[1]\n",
    "    let d: Int = input.shape()[2]\n",
    "    \n",
    "    # in_projection takes in input and outputs the input-specific Δ, B, C\n",
    "    let x_and_residual = in_projection(input)\n",
    "    \n",
    "    # split x and residual\n",
    "    # rearrange and do conv1d\n",
    "    # rearrange back\n",
    "    # silu activation\n",
    "    # y = ssx(x)\n",
    "    # y *= silu(residual)\n",
    "    # let output = out_projection(y)\n",
    "\n",
    "    return rand[DType.float32](4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn residual_block_forward(input: Tensor[DType.float32]) -> Tensor[DType.float32]:\n",
    "    alias simd_float32_width: Int = simdwidthof[DType.float32]()\n",
    "    var output = Tensor[DType.float32](input.shape())\n",
    "\n",
    "    let normalized_input = RMS_norm(input)\n",
    "    let mamba_forward = compute_mamba_block(normalized_input)\n",
    "\n",
    "    @parameter\n",
    "    fn residual_add[simd_float32_width: Int](idx: Int) -> None:\n",
    "        output.simd_store[simd_float32_width](idx, math.add(mamba_forward.simd_load[simd_float32_width](idx), \n",
    "                                                            input.simd_load[simd_float32_width](idx)))\n",
    "    vectorize[simd_float32_width, residual_add](output.num_elements())\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement this\n",
    "fn lm_head(input: Tensor[DType.float32]) -> Tensor[DType.float32]:\n",
    "    # return a random f32 tensor for now\n",
    "    return rand[DType.float32](input.shape())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement this\n",
    "fn mamba_forward_pass(input_ids: Tensor[DType.uint32]) -> Tensor[DType.float32]:\n",
    "    let num_layers = 2\n",
    "    var x = get_embedding(input_ids)\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        x = compute_layer(x)\n",
    "            \n",
    "    let normalized = RMS_norm(x)\n",
    "    let logits = lm_head(normalized)\n",
    "\n",
    "    return logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Mojo",
   "language": "mojo",
   "name": "mojo-jupyter-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "mojo"
   },
   "file_extension": ".mojo",
   "mimetype": "text/x-mojo",
   "name": "mojo"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
